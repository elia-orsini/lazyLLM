# a simple modelgraded eval checking if a completion is funny or not
joke-animals:
  id: joke-animals.dev.v0
  metrics: [accuracy]
joke-animals.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_multiio/battles/joke_animals_vs_fruits.jsonl
    samples_renamings:
      input1: "input"
      completion1: "completion"
    eval_type: cot_classify
    modelgraded_spec: humor

# (same eval as above, but with likert scale of 1-5)
joke-animals-likert:
  id: joke-animals-likert.dev.v0
  metrics: [accuracy]
joke-animals-likert.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_multiio/battles/joke_animals_vs_fruits.jsonl
    samples_renamings:
      input1: "input"
      completion1: "completion"
    eval_type: cot_classify
    modelgraded_spec: humor_likert

# a simple modelgraded eval checking if a completion is funny or not
# this example uses a labeled dataset, but ignores "completion" and "choice"
joke-fruits:
  id: joke-fruits.dev.v0
  metrics: [accuracy]
joke-fruits.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_metaeval/joke_fruits_labeled.jsonl
    eval_type: cot_classify
    modelgraded_spec: humor

# a meta-evaluation of a modelgraded eval checking if a completion is funny or not
# this example uses a labeled dataset with "completion" and "choice"
joke-fruits-meta:
  id: joke-fruits-meta.dev.v0
  metrics: [accuracy]
joke-fruits-meta.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_metaeval/joke_fruits_labeled.jsonl
    eval_type: cot_classify
    modelgraded_spec: humor
    metaeval: true

# (above, but with "answer then explain", rather than "reason then answer")
joke-fruits-expl-meta:
  id: joke-fruits-expl-meta.dev.v0
  metrics: [accuracy]
joke-fruits-expl-meta.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_metaeval/joke_fruits_labeled.jsonl
    eval_type: classify_cot
    modelgraded_spec: humor
    metaeval: true

# (above, but with "answer" only)
joke-fruits-ans-meta:
  id: joke-fruits-ans-meta.dev.v0
  metrics: [accuracy]
joke-fruits-ans-meta.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_metaeval/joke_fruits_labeled.jsonl
    eval_type: classify
    modelgraded_spec: humor
    metaeval: true

# a simple modelgraded eval checking if 4 completions to the sample prompt is diverse
# this example uses a labeled dataset, but ignore "completion" and "choice"
diversity:
  id: diversity.dev.v0
  metrics: [accuracy]
diversity.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_metaeval/joke_fruits_labeled.jsonl
    eval_type: cot_classify
    modelgraded_spec: diversity
    multicomp_n: 4
    multicomp_temperature: 0.4

# a simple modelgraded eval checking which of N completions to the sample prompt is the best response
# this example uses a labeled dataset, but ignore "completion" and "choice"
# command: `oaleval gpt-3.5-turbo,gpt-4 best`
best:
  id: best.dev.v0
  metrics: [accuracy]
best.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: test_metaeval/joke_fruits_labeled.jsonl
    eval_type: cot_classify
    modelgraded_spec: best
    multicomp_n: from_models
    multicomp_temperature: 0.0